{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to use UpTrain Framework to identify edge cases and retrain an orientation classification model to improve its accuracy. We are considering a task where given human pose (ie location of key-points such as nose, shoulders, wrist, hips, ankles etc.), the model tries to predict whether the person is in a vertical (ie standing) or a horizontal (ie lying) position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import uptrain\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from dataset import input_to_dataset_transformation, read_json, write_json, KpsDataset\n",
    "from pushup_signal import pushup_signal\n",
    "\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "remote_url = \"https://oodles-dev-training-data.s3.amazonaws.com/data.zip\"\n",
    "orig_training_file = 'data/training_data.json'\n",
    "if not os.path.exists(data_dir):\n",
    "    try:\n",
    "        file_downloaded_ok = subprocess.check_output(\"wget \" + remote_url, shell=True)\n",
    "    except:\n",
    "        print(\"Could not load training data\")\n",
    "    with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "\n",
    "    full_training_data = read_json(orig_training_file)\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(full_training_data)\n",
    "    reduced_training_data = full_training_data[0:1000]\n",
    "    write_json(orig_training_file, reduced_training_data)\n",
    "    \n",
    "training_file = 'data/training_data.json'\n",
    "golden_testing_file = 'data/golden_testing_data.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training with Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n"
     ]
    }
   ],
   "source": [
    "from model_logistic_regression import get_accuracy_lr, train_model_lr\n",
    "train_model_lr(training_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate the model on our golden testing dataset to see it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on  15731  data-points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8586231008836056"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_lr(golden_testing_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the testing accuracy of the model is quite low. We saw that on manual testing, model's outputs were unreliable in cases where we were in pushup position. Next, we will define the UpTrain config with edge-case check for Pushup signals. We also pass our training and evaluation arguments to facilitate automated retraining if a significant number of edge cases are detected.\n",
    "\n",
    "Let's define the data files: \n",
    "\n",
    "1. Real world test cases contains the data-points which the models sees in production. 2. Golden testing file is a testing dataset which we will use to compare performance of retrained model against originally deployed model. \n",
    "3. We want to log the collected data-points to a local folder defined in data save fold (this can also be a SQL table, a data warehouse etc.). \n",
    "4. To annotate the collected data points, we are extracting the Ground Truth from the master annotation file (this can also do something like schedule an annotation job on Mechanical turk or integrate with your other annotation pipelines). \n",
    "5. Finally, we define a Pushup signal which based on location of wrist, ankle and shoulder keypoints, estimate if the person is in pushup position. We use this signal to collect edge cases as based on manual testing, we saw our model's predictions are unreliable when we were lying upside down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_world_test_cases = 'data/real_world_testing_data.json'\n",
    "annotation_args = {'master_file': 'data/master_annotation_data.json'}\n",
    "data_save_fold = 'uptrain_smart_data__edge_cases'\n",
    "\n",
    "# Defining the egde-case signal\n",
    "pushup_edge_case = uptrain.Signal(\"Pushup\", pushup_signal)\n",
    "\n",
    "cfg = {\n",
    "    # Define your signal to identify edge cases\n",
    "    \"checks\": [{\n",
    "        'type': uptrain.Anomaly.EDGE_CASE, \n",
    "        \"signal_formulae\": pushup_edge_case\n",
    "    }],\n",
    "    \n",
    "    # Will use this as the primary key to reference individual data-points\n",
    "    \"data_identifier\": \"id\",\n",
    "\n",
    "    # Connect training pipeline to annotate data and retrain the model\n",
    "    \"training_args\": {\n",
    "        \"data_transformation_func\": input_to_dataset_transformation,  \n",
    "        \"annotation_method\": {\"method\": uptrain.AnnotationMethod.MASTER_FILE, \"args\": annotation_args}, \n",
    "        \"training_func\": train_model_lr, \n",
    "        \"fold_name\": data_save_fold,\n",
    "        \"orig_training_file\": orig_training_file,  \n",
    "    },\n",
    "\n",
    "    # Retrain once 250 edge cases are collected\n",
    "    \"retrain_after\": 250,\n",
    "\n",
    "    # Connect evaluation pipeline to test retrained model against original model\n",
    "    \"evaluation_args\": {\n",
    "        \"inference_func\": get_accuracy_lr,\n",
    "        \"golden_testing_dataset\": golden_testing_file,\n",
    "        \"metrics_to_check\": ['accuracy']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To integrate UpTrain, we need to just initialise a Framework object with above-defined config and log model inputs and outputs in our inference function. \n",
    "\n",
    "To mimic real-world settings, we take a real-world testing dataset, load data-points batch by batch and run the model inference on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_logs\n"
     ]
    }
   ],
   "source": [
    "framework_lr = uptrain.Framework(cfg)\n",
    "\n",
    "testing_dataset = KpsDataset(real_world_test_cases, normalization=True)\n",
    "X_test, y_test, id = testing_dataset.load_x_y_from_data()\n",
    "inference_batch_size = 256\n",
    "pred_classes = []\n",
    "model = joblib.load(\"trained_models_lr/\" + 'version_0')\n",
    "for i in range(int(np.ceil(len(X_test)/inference_batch_size))):\n",
    "\n",
    "    if i==0:\n",
    "        break\n",
    "    \n",
    "    # Do model prediction\n",
    "    elem = X_test[i*inference_batch_size:min((i+1)*inference_batch_size,len(X_test))]\n",
    "    ids = id[i*inference_batch_size:min((i+1)*inference_batch_size,len(X_test))]\n",
    "    inputs = {\"data\": {\"kps\": elem}, \"id\": ids}\n",
    "    preds = model.predict(inputs['data']['kps'])\n",
    "\n",
    "    # Log model inputs and outputs to the uptrain Framework\n",
    "    idens = framework_lr.log(inputs=inputs, outputs=preds)\n",
    "\n",
    "    # Retrain only once\n",
    "    if framework_lr.version > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the comparison report above, we can see how UpTrain improved the model performance by detecting edge-cases and retraining the model under-the-hood. Further, UpTrain is agnostic to the model type and training functions. To illustrate this, we again train our orientation classification model, but this time with Deep Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Deep Neural Network (with PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from model_torch import get_accuracy_torch, train_model_torch, BinaryClassification\n",
    "train_model_torch('data/training_data.json', 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the model accuracy on testing dataset, which is again low due to misclassification of Pushup signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on  15731  data-points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9086517068209269"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_torch(golden_testing_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the UpTrain config with new training workflows and checks. Let's also add a check for edge-cases when model confidence is low (because why not!). For binary entropy confidence, we can directly use one of the pre-defined model signals and adjust the confidence threshold according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whenever model confidence is <0.9, identify it as an edge-case \n",
    "low_conf_edge_case = uptrain.Signal(uptrain.ModelSignal.BINARY_ENTROPY_CONFIDENCE, \n",
    "                is_model_signal=True) < 0.9\n",
    "\n",
    "cfg['checks'][0].update({\"signal_formulae\": (pushup_edge_case | low_conf_edge_case)})\n",
    "cfg['training_args'].update({'training_func': train_model_torch})\n",
    "cfg['evaluation_args'].update({'inference_func': get_accuracy_torch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_smart_data__edge_cases\n",
      "Deleting the folder:  uptrain_logs\n",
      "55  edge-cases collected out of  208  inferred samples\n",
      "100  edge-cases collected out of  416  inferred samples\n",
      "151  edge-cases collected out of  624  inferred samples\n",
      "206  edge-cases collected out of  832  inferred samples\n",
      "250  edge-cases collected out of  992  inferred samples\n",
      "Kicking off re-training\n",
      "255 data-points selected out of 1008\n",
      "Training on:  uptrain_smart_data__edge_cases/1/training_dataset.json  which has  2275  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Model retraining done...\n",
      "Generating comparison report...\n",
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Evaluating on  15731  data-points\n",
      "Evaluating on  15731  data-points\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Old model accuracy:  0.9086517068209269\n",
      "Retrained model accuracy (ie 255 smartly collected data-points added):  0.992498887546882\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "framework_torch = uptrain.Framework(cfg)\n",
    "\n",
    "inference_batch_size = 16\n",
    "model_dir = 'trained_models_torch/'\n",
    "model_save_name = 'version_0'\n",
    "real_world_dataset = KpsDataset(\n",
    "    real_world_test_cases, batch_size=inference_batch_size, shuffle=False, augmentations=False, is_test=True\n",
    ")\n",
    "model = BinaryClassification()\n",
    "model.load_state_dict(torch.load(model_dir + model_save_name))\n",
    "model.eval()\n",
    "gt_data = read_json(annotation_args['master_file'])\n",
    "all_gt_ids = [x['id'] for x in gt_data]\n",
    "\n",
    "for i,elem in enumerate(real_world_dataset):\n",
    "\n",
    "    # Do model prediction\n",
    "    inputs = {\"data\": {\"kps\": elem[0][\"kps\"]}, \"id\": elem[0][\"id\"]}\n",
    "    x_test = torch.tensor(inputs[\"data\"][\"kps\"]).type(torch.float)\n",
    "    test_logits = model(x_test).squeeze() \n",
    "    preds = torch.round(torch.sigmoid(test_logits)).detach().numpy()\n",
    "    idens = framework_torch.log(inputs=inputs, outputs=preds)\n",
    "\n",
    "    # Attach ground truth\n",
    "    this_elem_gt = [gt_data[all_gt_ids.index(x)]['gt'] for x in elem[0]['id']]\n",
    "    framework_torch.log(identifiers=idens, gts=this_elem_gt)\n",
    "\n",
    "    # Retrain only once\n",
    "    if framework_torch.version > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Deep Neural Network (with Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "from model_dnn import get_accuracy_dnn, train_model_dnn\n",
    "train_model_dnn('data/training_data.json', 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the model accuracy on testing dataset, which is again low due to misclassification of Pushup signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 237us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-14 11:46:41.332360: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9282944504481597"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_dnn(golden_testing_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the UpTrain config with new training workflows and checks. Let's also add a check for edge-cases when model confidence is low (because why not!). For binary entropy confidence, we can directly use one of the pre-defined model signals and adjust the confidence threshold according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whenever model confidence is <0.9, identify it as an edge-case \n",
    "low_conf_edge_case = uptrain.Signal(uptrain.ModelSignal.BINARY_ENTROPY_CONFIDENCE, \n",
    "                is_model_signal=True) < 0.9\n",
    "\n",
    "cfg['checks'][0].update({\"signal_formulae\": (pushup_edge_case | low_conf_edge_case)})\n",
    "cfg['training_args'].update({'training_func': train_model_dnn})\n",
    "cfg['evaluation_args'].update({'inference_func': get_accuracy_dnn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_smart_data__edge_cases\n",
      "Deleting the folder:  uptrain_logs\n",
      "52  edge-cases collected out of  176  inferred samples\n",
      "102  edge-cases collected out of  384  inferred samples\n",
      "152  edge-cases collected out of  592  inferred samples\n",
      "200  edge-cases collected out of  800  inferred samples\n",
      "252  edge-cases collected out of  960  inferred samples\n",
      "Kicking off re-training\n",
      "252 data-points selected out of 960\n",
      "Training on:  uptrain_smart_data__edge_cases/1/training_dataset.json  which has  2260  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Model retraining done...\n",
      "Generating comparison report...\n",
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 201us/step\n",
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 217us/step\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Old model accuracy:  0.9282944504481597\n",
      "Retrained model accuracy (ie 252 smartly collected data-points added):  0.9727925751700464\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "framework_dnn = uptrain.Framework(cfg)\n",
    "\n",
    "model_dir = 'trained_models_dnn/'\n",
    "model_save_name = 'version_0'\n",
    "inference_batch_size = 16\n",
    "real_world_dataset = KpsDataset(\n",
    "    real_world_test_cases, batch_size=inference_batch_size, shuffle=False, augmentations=False, is_test=True\n",
    ")\n",
    "model = tf.keras.models.load_model(model_dir + model_save_name)\n",
    "gt_data = read_json(annotation_args['master_file'])\n",
    "all_gt_ids = [x['id'] for x in gt_data]\n",
    "\n",
    "for i,elem in enumerate(real_world_dataset):\n",
    "\n",
    "    # Do model prediction\n",
    "    inputs = {\"data\": {\"kps\": elem[0][\"kps\"]}, \"id\": elem[0][\"id\"]}\n",
    "    with open('evaluation_logs.txt', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            preds = model.predict(inputs['data']['kps'])\n",
    "\n",
    "    # Log model inputs and outputs to the uptrain Framework\n",
    "    idens = framework_dnn.log(inputs=inputs, outputs=preds)\n",
    "\n",
    "    # Retrain only once\n",
    "    if framework_dnn.version > 1:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
