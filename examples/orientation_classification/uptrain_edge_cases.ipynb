{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to use UpTrain Framework to identify edge cases and retrain an orientation classification model to improve its accuracy. We are considering a task where given human pose (ie location of key-points such as nose, shoulders, wrist, hips, ankles etc.), the model tries to predict whether the person is in a vertical (ie standing) or a horizontal (ie lying) position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import uptrain\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from dataset import input_to_dataset_transformation, read_json, write_json, KpsDataset\n",
    "from pushup_signal import pushup_signal\n",
    "\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "remote_url = \"https://oodles-dev-training-data.s3.amazonaws.com/data.zip\"\n",
    "orig_training_file = 'data/training_data.json'\n",
    "if not os.path.exists(data_dir):\n",
    "    try:\n",
    "        file_downloaded_ok = subprocess.check_output(\"wget \" + remote_url, shell=True)\n",
    "    except:\n",
    "        print(\"Could not load training data\")\n",
    "    with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "\n",
    "    full_training_data = read_json(orig_training_file)\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(full_training_data)\n",
    "    reduced_training_data = full_training_data[0:1000]\n",
    "    write_json(orig_training_file, reduced_training_data)\n",
    "    \n",
    "training_file = 'data/training_data.json'\n",
    "golden_testing_file = 'data/golden_testing_data.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training with Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Model saved at:  trained_models_lr/version_0\n"
     ]
    }
   ],
   "source": [
    "from model_logistic_regression import get_accuracy_lr, train_model_lr\n",
    "train_model_lr(training_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate the model on our golden testing dataset to see it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on  15731  data-points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8586231008836056"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_lr(golden_testing_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the testing accuracy of the model is quite low. We saw that on manual testing, model's outputs were unreliable in cases where we were in pushup position. Next, we will define the UpTrain config with edge-case check for Pushup signals. We also pass our training and evaluation arguments to facilitate automated retraining if a significant number of edge cases are detected.\n",
    "\n",
    "Let's define the data files: \n",
    "\n",
    "1. Real world test cases contains the data-points which the models sees in production. 2. Golden testing file is a testing dataset which we will use to compare performance of retrained model against originally deployed model. \n",
    "3. We want to log the collected data-points to a local folder defined in data save fold (this can also be a SQL table, a data warehouse etc.). \n",
    "4. To annotate the collected data points, we are extracting the Ground Truth from the master annotation file (this can also do something like schedule an annotation job on Mechanical turk or integrate with your other annotation pipelines). \n",
    "5. Finally, we define a Pushup signal which based on location of wrist, ankle and shoulder keypoints, estimate if the person is in pushup position. We use this signal to collect edge cases as based on manual testing, we saw our model's predictions are unreliable when we were lying upside down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_world_test_cases = 'data/real_world_testing_data.json'\n",
    "annotation_args = {'master_file': 'data/master_annotation_data.json'}\n",
    "data_save_fold = 'uptrain_smart_data__edge_cases'\n",
    "\n",
    "# Defining the egde-case signal\n",
    "pushup_edge_case = uptrain.Signal(\"Pushup\", pushup_signal)\n",
    "\n",
    "cfg = {\n",
    "    # Define your signal to identify edge cases\n",
    "    \"checks\": [{\n",
    "        'type': uptrain.Anomaly.EDGE_CASE, \n",
    "        \"signal_formulae\": pushup_edge_case\n",
    "    }],\n",
    "    \n",
    "    # Will use this as the primary key to reference individual data-points\n",
    "    \"data_identifier\": \"id\",\n",
    "\n",
    "    # Connect training pipeline to annotate data and retrain the model\n",
    "    \"training_args\": {\n",
    "        \"data_transformation_func\": input_to_dataset_transformation,  \n",
    "        \"annotation_method\": {\"method\": uptrain.AnnotationMethod.MASTER_FILE, \"args\": annotation_args}, \n",
    "        \"training_func\": train_model_lr, \n",
    "        \"fold_name\": data_save_fold,\n",
    "        \"orig_training_file\": orig_training_file,  \n",
    "    },\n",
    "\n",
    "    # Retrain once 250 edge cases are collected\n",
    "    \"retrain_after\": 250,\n",
    "\n",
    "    # Connect evaluation pipeline to test retrained model against original model\n",
    "    \"evaluation_args\": {\n",
    "        \"inference_func\": get_accuracy_lr,\n",
    "        \"golden_testing_dataset\": golden_testing_file,\n",
    "        \"metrics_to_check\": ['accuracy']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To integrate UpTrain, we need to just initialise a Framework object with above-defined config and log model inputs and outputs in our inference function. \n",
    "\n",
    "To mimic real-world settings, we take a real-world testing dataset, load data-points batch by batch and run the model inference on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_smart_data__edge_cases\n",
      "Deleting the folder:  uptrain_logs\n",
      "132  edge-cases collected out of  256  inferred samples\n",
      "246  edge-cases collected out of  512  inferred samples\n",
      "368  edge-cases collected out of  768  inferred samples\n",
      "Kicking off re-training\n",
      "368 data-points selected out of 768\n",
      "Training on:  uptrain_smart_data__edge_cases/1/training_dataset.json  which has  2840  data-points\n",
      "Model saved at:  trained_models_lr/version_1\n",
      "Model retraining done...\n",
      "Generating comparison report...\n",
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Evaluating on  15731  data-points\n",
      "Evaluating on  15731  data-points\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Old model accuracy:  0.8586231008836056\n",
      "Retrained model accuracy (ie 368 smartly collected data-points added):  0.9691055876930901\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "framework_lr = uptrain.Framework(cfg)\n",
    "\n",
    "testing_dataset = KpsDataset(real_world_test_cases, normalization=True)\n",
    "X_test, y_test, id = testing_dataset.load_x_y_from_data()\n",
    "inference_batch_size = 256\n",
    "pred_classes = []\n",
    "model = joblib.load(\"trained_models_lr/\" + 'version_0')\n",
    "for i in range(int(np.ceil(len(X_test)/inference_batch_size))): \n",
    "    # Do model prediction\n",
    "    elem = X_test[i*inference_batch_size:min((i+1)*inference_batch_size,len(X_test))]\n",
    "    ids = id[i*inference_batch_size:min((i+1)*inference_batch_size,len(X_test))]\n",
    "    inputs = {\"data\": {\"kps\": elem}, \"id\": ids}\n",
    "    preds = model.predict(inputs['data']['kps'])\n",
    "\n",
    "    # Log model inputs and outputs to the uptrain Framework\n",
    "    idens = framework_lr.log(inputs=inputs, outputs=preds)\n",
    "\n",
    "    # Retrain only once\n",
    "    if framework_lr.version > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the comparison report above, we can see how UpTrain improved the model performance by detecting edge-cases and retraining the model under-the-hood. Further, UpTrain is agnostic to the model type and training functions. To illustrate this, we again train our orientation classification model, but this time with Deep Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Deep Neural Network (with PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Epoch 0: Loss 4.843758450400445\n",
      "Epoch 1: Loss 1.3056830919407834\n",
      "Epoch 2: Loss 0.963068675014696\n",
      "Epoch 3: Loss 0.9446813049870534\n",
      "Epoch 4: Loss 0.8719852773281044\n",
      "Epoch 5: Loss 0.5867635330158067\n",
      "Epoch 6: Loss 0.48831840431328954\n",
      "Epoch 7: Loss 0.5206127582173541\n",
      "Epoch 8: Loss 0.3507806431908619\n",
      "Epoch 9: Loss 0.29904839011273465\n",
      "Model saved at:  trained_models_torch/version_0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from model_torch import get_accuracy_torch, train_model_torch, BinaryClassification\n",
    "train_model_torch('data/training_data.json', 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the model accuracy on testing dataset, which is again low due to misclassification of Pushup signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on  15731  data-points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.946792956582544"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_torch(golden_testing_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the UpTrain config with new training workflows and checks. Let's also add a check for edge-cases when model confidence is low (because why not!). For binary entropy confidence, we can directly use one of the pre-defined model signals and adjust the confidence threshold according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whenever model confidence is <0.9, identify it as an edge-case \n",
    "low_conf_edge_case = uptrain.Signal(uptrain.ModelSignal.BINARY_ENTROPY_CONFIDENCE, \n",
    "                is_model_signal=True) < 0.9\n",
    "\n",
    "cfg['checks'][0].update({\"signal_formulae\": (pushup_edge_case | low_conf_edge_case)})\n",
    "cfg['training_args'].update({'training_func': train_model_torch})\n",
    "cfg['evaluation_args'].update({'inference_func': get_accuracy_torch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_smart_data__edge_cases\n",
      "Deleting the folder:  uptrain_logs\n",
      "55  edge-cases collected out of  208  inferred samples\n",
      "100  edge-cases collected out of  416  inferred samples\n",
      "151  edge-cases collected out of  624  inferred samples\n",
      "206  edge-cases collected out of  832  inferred samples\n",
      "250  edge-cases collected out of  992  inferred samples\n",
      "Kicking off re-training\n",
      "255 data-points selected out of 1008\n",
      "Training on:  uptrain_smart_data__edge_cases/1/training_dataset.json  which has  2275  data-points\n",
      "Epoch 0: Loss 4.456094716983834\n",
      "Epoch 1: Loss 1.5041649351908155\n",
      "Epoch 2: Loss 0.7094659108056898\n",
      "Epoch 3: Loss 0.5479656164785824\n",
      "Epoch 4: Loss 0.44732532045410295\n",
      "Epoch 5: Loss 0.4169701081771625\n",
      "Epoch 6: Loss 0.38897821375919295\n",
      "Epoch 7: Loss 0.2797425045332354\n",
      "Epoch 8: Loss 0.23910698301602706\n",
      "Epoch 9: Loss 0.21545567348820582\n",
      "Model saved at:  trained_models_torch/version_1\n",
      "Model retraining done...\n",
      "Generating comparison report...\n",
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Evaluating on  15731  data-points\n",
      "Evaluating on  15731  data-points\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Old model accuracy:  0.946792956582544\n",
      "Retrained model accuracy (ie 255 smartly collected data-points added):  0.9886847625707202\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "framework_torch = uptrain.Framework(cfg)\n",
    "\n",
    "inference_batch_size = 16\n",
    "model_dir = 'trained_models_torch/'\n",
    "model_save_name = 'version_0'\n",
    "real_world_dataset = KpsDataset(\n",
    "    real_world_test_cases, batch_size=inference_batch_size, shuffle=False, augmentations=False, is_test=True\n",
    ")\n",
    "model = BinaryClassification()\n",
    "model.load_state_dict(torch.load(model_dir + model_save_name))\n",
    "model.eval()\n",
    "gt_data = read_json(annotation_args['master_file'])\n",
    "all_gt_ids = [x['id'] for x in gt_data]\n",
    "\n",
    "for i,elem in enumerate(real_world_dataset):\n",
    "\n",
    "    # Do model prediction\n",
    "    inputs = {\"data\": {\"kps\": elem[0][\"kps\"]}, \"id\": elem[0][\"id\"]}\n",
    "    x_test = torch.tensor(inputs[\"data\"][\"kps\"]).type(torch.float)\n",
    "    test_logits = model(x_test).squeeze() \n",
    "    preds = torch.round(torch.sigmoid(test_logits)).detach().numpy()\n",
    "    idens = framework_torch.log(inputs=inputs, outputs=preds)\n",
    "\n",
    "    # Attach ground truth\n",
    "    this_elem_gt = [gt_data[all_gt_ids.index(x)]['gt'] for x in elem[0]['id']]\n",
    "    framework_torch.log(identifiers=idens, gts=this_elem_gt)\n",
    "\n",
    "    # Retrain only once\n",
    "    if framework_torch.version > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Deep Neural Network (with Tensorflow), \n",
    "\n",
    "Note: Requires tensorflow to be installed. We ran the following code successfully with tf version 2.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  data/training_data.json  which has  1000  data-points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-15 22:32:39.163172: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "62/62 [==============================] - 0s 612us/step - loss: 79.1203 - binary_accuracy: 0.3730\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 0s 550us/step - loss: 18.0154 - binary_accuracy: 0.5383\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 0s 526us/step - loss: 5.8314 - binary_accuracy: 0.7470\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 0s 513us/step - loss: 2.9454 - binary_accuracy: 0.8488\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 0s 516us/step - loss: 2.1988 - binary_accuracy: 0.8780\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 0s 515us/step - loss: 1.9223 - binary_accuracy: 0.8871\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 0s 519us/step - loss: 1.7631 - binary_accuracy: 0.9062\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 0s 515us/step - loss: 1.6675 - binary_accuracy: 0.9093\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 0s 519us/step - loss: 1.5129 - binary_accuracy: 0.9093\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 0s 513us/step - loss: 1.4336 - binary_accuracy: 0.9194\n",
      "INFO:tensorflow:Assets written to: trained_models_tf/version_0/assets\n",
      "Model saved at:  trained_models_tf/version_0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "from model_tf import get_accuracy_tf, train_model_tf\n",
    "train_model_tf('data/training_data.json', 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the model accuracy on testing dataset, which is again low due to misclassification of Pushup signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 350us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9422795753607527"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_tf(golden_testing_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the UpTrain config with new training workflows and checks. Let's also add a check for edge-cases when model confidence is low (because why not!). For binary entropy confidence, we can directly use one of the pre-defined model signals and adjust the confidence threshold according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whenever model confidence is <0.9, identify it as an edge-case \n",
    "low_conf_edge_case = uptrain.Signal(uptrain.ModelSignal.BINARY_ENTROPY_CONFIDENCE, \n",
    "                is_model_signal=True) < 0.9\n",
    "\n",
    "cfg['checks'][0].update({\"signal_formulae\": (pushup_edge_case | low_conf_edge_case)})\n",
    "cfg['training_args'].update({'training_func': train_model_tf})\n",
    "cfg['evaluation_args'].update({'inference_func': get_accuracy_tf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_smart_data__edge_cases\n",
      "Deleting the folder:  uptrain_logs\n",
      "50  edge-cases collected out of  192  inferred samples\n",
      "101  edge-cases collected out of  400  inferred samples\n",
      "150  edge-cases collected out of  608  inferred samples\n",
      "202  edge-cases collected out of  816  inferred samples\n",
      "252  edge-cases collected out of  976  inferred samples\n",
      "Kicking off re-training\n",
      "252 data-points selected out of 976\n",
      "Training on:  uptrain_smart_data__edge_cases/1/training_dataset.json  which has  2260  data-points\n",
      "Epoch 1/10\n",
      "141/141 [==============================] - 0s 547us/step - loss: 38.9979 - binary_accuracy: 0.5638\n",
      "Epoch 2/10\n",
      "141/141 [==============================] - 0s 501us/step - loss: 6.5352 - binary_accuracy: 0.8054\n",
      "Epoch 3/10\n",
      "141/141 [==============================] - 0s 521us/step - loss: 2.6563 - binary_accuracy: 0.8808\n",
      "Epoch 4/10\n",
      "141/141 [==============================] - 0s 510us/step - loss: 1.8180 - binary_accuracy: 0.9025\n",
      "Epoch 5/10\n",
      "141/141 [==============================] - 0s 501us/step - loss: 1.3284 - binary_accuracy: 0.9242\n",
      "Epoch 6/10\n",
      "141/141 [==============================] - 0s 503us/step - loss: 1.1194 - binary_accuracy: 0.9313\n",
      "Epoch 7/10\n",
      "141/141 [==============================] - 0s 510us/step - loss: 0.9591 - binary_accuracy: 0.9384\n",
      "Epoch 8/10\n",
      "141/141 [==============================] - 0s 504us/step - loss: 0.8411 - binary_accuracy: 0.9433\n",
      "Epoch 9/10\n",
      "141/141 [==============================] - 0s 492us/step - loss: 0.7836 - binary_accuracy: 0.9490\n",
      "Epoch 10/10\n",
      "141/141 [==============================] - 0s 500us/step - loss: 0.7556 - binary_accuracy: 0.9468\n",
      "INFO:tensorflow:Assets written to: trained_models_tf/version_1/assets\n",
      "Model saved at:  trained_models_tf/version_1\n",
      "Model retraining done...\n",
      "Generating comparison report...\n",
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 343us/step\n",
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 344us/step\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Old model accuracy:  0.9422795753607527\n",
      "Retrained model accuracy (ie 252 smartly collected data-points added):  0.973301125166868\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "framework_tf = uptrain.Framework(cfg)\n",
    "\n",
    "model_dir = 'trained_models_tf/'\n",
    "model_save_name = 'version_0'\n",
    "inference_batch_size = 16\n",
    "real_world_dataset = KpsDataset(\n",
    "    real_world_test_cases, batch_size=inference_batch_size, shuffle=False, augmentations=False, is_test=True\n",
    ")\n",
    "model = tf.keras.models.load_model(model_dir + model_save_name)\n",
    "gt_data = read_json(annotation_args['master_file'])\n",
    "all_gt_ids = [x['id'] for x in gt_data]\n",
    "\n",
    "for i,elem in enumerate(real_world_dataset):\n",
    "\n",
    "    # Do model prediction\n",
    "    inputs = {\"data\": {\"kps\": elem[0][\"kps\"]}, \"id\": elem[0][\"id\"]}\n",
    "    with open('evaluation_logs.txt', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            preds = model.predict(inputs['data']['kps'])\n",
    "\n",
    "    # Log model inputs and outputs to the uptrain Framework\n",
    "    idens = framework_tf.log(inputs=inputs, outputs=preds)\n",
    "\n",
    "    # Retrain only once\n",
    "    if framework_tf.version > 1:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
