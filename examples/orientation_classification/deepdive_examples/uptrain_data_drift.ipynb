{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how we can use UpTrain package to identify data drift and identify out of distribution cases for the same orientation classification example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import uptrain\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from model_files import input_to_dataset_transformation, read_json, write_json, KpsDataset\n",
    "from model_files import body_length_signal, plot_all_cluster\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "remote_url = \"https://oodles-dev-training-data.s3.amazonaws.com/data.zip\"\n",
    "orig_training_file = 'data/training_data.json'\n",
    "if not os.path.exists(data_dir):\n",
    "    try:\n",
    "        # Most Linux distributions have Wget installed by default.\n",
    "        # Below command is to install wget for MacOS\n",
    "        wget_installed_ok = subprocess.call(\"brew install wget\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "        print(\"Successfully installed wget\")\n",
    "    except:\n",
    "        dummy = 1\n",
    "    try:\n",
    "        if not os.path.exists(\"data.zip\"):\n",
    "            file_downloaded_ok = subprocess.call(\"wget \" + remote_url, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "            print(\"Data downloaded\")\n",
    "        with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./\")\n",
    "        full_training_data = read_json(orig_training_file)\n",
    "        np.random.seed(1)\n",
    "        np.random.shuffle(full_training_data)\n",
    "        reduced_training_data = full_training_data[0:1000]\n",
    "        write_json(orig_training_file, reduced_training_data)\n",
    "        print(\"Prepared Example Dataset\")\n",
    "        os.remove(\"data.zip\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Could not load training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_world_test_cases = 'data/real_world_testing_data.json'\n",
    "golden_testing_file = 'data/golden_testing_data.json'\n",
    "annotation_args = {'master_file': 'data/master_annotation_data.json'}\n",
    "\n",
    "inference_batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our network using Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n"
     ]
    }
   ],
   "source": [
    "from model_files import get_accuracy_torch, train_model_torch, BinaryClassification\n",
    "train_model_torch('data/training_data.json', 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the model accuracy on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on  15731  data-points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9184412942597419"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_torch(golden_testing_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the UpTrain config to identify data drifts. We define 3 checks here:\n",
    "\n",
    "1. Data Drift for input features - keypoints: Keypoints is a 34-dimensional vector (x,y for 17 body joints). We will use Embedding based clustering to calculate Earth Moving Distance to identify if we see data distributions very different from the reference dataset (ie original training file). Additionally, it also collects the edge datapoints.\n",
    "\n",
    "2. Data Drift for only the first feature - X co-ordinate of the Head keypoint. In addition to the overall input, we also want to see specific features in the input are drifting or not. To specify the same, we defined a complex measurable to extract scalar from the input keypoints embeddings.\n",
    "\n",
    "3. Data Drift for Body Length: Many of the times, functions on the input features might be more meaningful to look as compared to the raw features. In this case, we can use the location of the keypoints to determine body length of the user using the UpTrain Signals and monitor if we see any shifts in it's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"checks\": [{\n",
    "        'type': uptrain.Anomaly.DATA_DRIFT,\n",
    "        'reference_dataset': orig_training_file,\n",
    "        'is_embedding': True,\n",
    "        'cluster_plot_func': plot_all_cluster,\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.Anomaly.DATA_DRIFT,\n",
    "        'reference_dataset': orig_training_file,\n",
    "        \"save_edge_cases\": False,\n",
    "        \"measurable_args\": {\n",
    "            'type': uptrain.MeasurableType.SCALAR_FROM_EMBEDDING,\n",
    "            'idx': 0,\n",
    "            'extract_from': {\n",
    "                'type': uptrain.MeasurableType.INPUT_FEATURE,\n",
    "                'feature_name': 'kps'\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.Anomaly.DATA_DRIFT,\n",
    "        'reference_dataset': orig_training_file,\n",
    "        'is_embedding': False,\n",
    "        \"save_edge_cases\": False,\n",
    "        \"measurable_args\": {\n",
    "            'type': uptrain.MeasurableType.CUSTOM,\n",
    "            'signal_formulae': uptrain.Signal(\"Body Length\", body_length_signal),\n",
    "        }\n",
    "    }],\n",
    "    \"data_identifier\": \"id\",\n",
    "    \"feat_name_list\": [\"kps\"],\n",
    "\n",
    "    # Connect training pipeline to annotate data and retrain the model\n",
    "    \"training_args\": {\n",
    "        \"data_transformation_func\": input_to_dataset_transformation,  \n",
    "        \"annotation_method\": {\"method\": uptrain.AnnotationMethod.MASTER_FILE, \"args\": annotation_args}, \n",
    "        \"training_func\": train_model_torch, \n",
    "        \"fold_name\": \"uptrain_smart_data__data_drift\",\n",
    "        \"orig_training_file\": orig_training_file,\n",
    "        \"cluster_plot_func\": plot_all_cluster\n",
    "    },\n",
    "\n",
    "    # Retrain once 250 edge cases are collected\n",
    "    \"retrain_after\": 100,\n",
    "\n",
    "    # Connect evaluation pipeline to test retrained model against original model\n",
    "    \"evaluation_args\": {\n",
    "        \"inference_func\": get_accuracy_torch,\n",
    "        \"golden_testing_dataset\": golden_testing_file,\n",
    "        \"metrics_to_check\": ['accuracy']\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_logs\n"
     ]
    }
   ],
   "source": [
    "framework_torch = uptrain.Framework(cfg)\n",
    "\n",
    "model_dir = 'trained_models_torch/'\n",
    "model_save_name = 'version_0'\n",
    "real_world_dataset = KpsDataset(\n",
    "    real_world_test_cases, batch_size=inference_batch_size, is_test=True\n",
    ")\n",
    "model = BinaryClassification()\n",
    "model.load_state_dict(torch.load(model_dir + model_save_name))\n",
    "model.eval()\n",
    "\n",
    "for i,elem in enumerate(real_world_dataset):\n",
    "\n",
    "    # Do model prediction\n",
    "    inputs = {\"data\": {\"kps\": elem[0][\"kps\"]}, \"id\": elem[0][\"id\"]}\n",
    "    x_test = torch.tensor(inputs[\"data\"][\"kps\"]).type(torch.float)\n",
    "    test_logits = model(x_test).squeeze() \n",
    "    preds = torch.round(torch.sigmoid(test_logits)).detach().numpy()\n",
    "\n",
    "    # Log model inputs and outputs to the uptrain Framework to monitor data drift\n",
    "    idens = framework_torch.log(inputs=inputs, outputs=preds)\n",
    "\n",
    "    # Retrain only once\n",
    "    if framework_torch.version > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.11.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir uptrain_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
