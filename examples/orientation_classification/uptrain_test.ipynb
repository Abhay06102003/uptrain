{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how we can use UpTrain package to observe model performance, data distributions, data integrity and identify edge cases to retrain an orientation classification model to improve it's accuracy. We are considering a task where given human pose (ie location of key-points such as nose, shoulders, wrist, hips, ankles etc.), the model tries to predict whether the person is in a vertical (ie standing) or a horizontal (ie lying) position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import uptrain\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from model_files import input_to_dataset_transformation, read_json, write_json, KpsDataset\n",
    "from model_files import body_length_signal, pushup_signal, plot_all_cluster\n",
    "\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "remote_url = \"https://oodles-dev-training-data.s3.amazonaws.com/data.zip\"\n",
    "orig_training_file = 'data/training_data.json'\n",
    "if not os.path.exists(data_dir):\n",
    "    try:\n",
    "        # Most Linux distributions have Wget installed by default.\n",
    "        # Below command is to install wget for MacOS\n",
    "        wget_installed_ok = subprocess.call(\"brew install wget\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "        print(\"Successfully installed wget\")\n",
    "    except:\n",
    "        dummy = 1\n",
    "    try:\n",
    "        if not os.path.exists(\"data.zip\"):\n",
    "            file_downloaded_ok = subprocess.call(\"wget \" + remote_url, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "            print(\"Data downloaded\")\n",
    "        with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"./\")\n",
    "        full_training_data = read_json(orig_training_file)\n",
    "        np.random.seed(1)\n",
    "        np.random.shuffle(full_training_data)\n",
    "        reduced_training_data = full_training_data[0:1000]\n",
    "        write_json(orig_training_file, reduced_training_data)\n",
    "        print(\"Prepared Example Dataset\")\n",
    "        os.remove(\"data.zip\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Could not load training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the data files: \n",
    "\n",
    "1. Real world test cases contains the data-points which the models sees in production. \n",
    "2. Golden testing file is a testing dataset which we will use to compare performance of retrained model against originally deployed model. \n",
    "3. We want to log the collected data-points to a local folder defined in data save fold (this can also be a SQL table, a data warehouse etc.). \n",
    "4. To annotate the collected data points, we are extracting the Ground Truth from the master annotation file (this can also do something like schedule an annotation job on Mechanical turk or integrate with your other annotation pipelines). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_world_test_cases = 'data/real_world_testing_data.json'\n",
    "golden_testing_file = 'data/golden_testing_data.json'\n",
    "data_save_fold = \"uptrain_smart_data\"\n",
    "\n",
    "inference_batch_size = 16\n",
    "annotation_args = {'master_file': 'data/master_annotation_data.json'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our network using Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n"
     ]
    }
   ],
   "source": [
    "from model_files import get_accuracy_torch, train_model_torch, BinaryClassification\n",
    "train_model_torch('data/training_data.json', 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate the model on our golden testing dataset to see it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on  15731  data-points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8841777382238891"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_torch(golden_testing_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the UpTrain config  We also pass our training and evaluation arguments to facilitate automated retraining if a significant number of edge cases are detected.\n",
    "\n",
    "Let's define the UpTrain config with following checks:\n",
    "\n",
    "1. Data Drift for input features - keypoints: Keypoints is a 34-dimensional vector (x,y for 17 body joints). We will use Embedding based clustering to calculate Earth Moving Distance to identify if we see data distributions very different from the reference dataset (ie original training file). Additionally, it also collects the edge datapoints.\n",
    "\n",
    "2. Data Integrity - Check if body length (a custom defined metric) is greater than 100\n",
    "\n",
    "3. Edge cases - We define a Pushup signal which based on location of wrist, ankle and shoulder keypoints, estimate if the person is in pushup position. We use this signal to collect edge cases as based on manual testing, we saw our model's predictions are unreliable when we were lying upside down.\n",
    "\n",
    "4. Concept Drift - We want to monitor degradation in model's performance. We will use DDM (Drift Detection Method) for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"checks\": [\n",
    "    {\n",
    "        'type': uptrain.Anomaly.DATA_DRIFT,\n",
    "        'reference_dataset': orig_training_file,\n",
    "        'is_embedding': True,\n",
    "        'cluster_plot_func': plot_all_cluster,\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.Anomaly.DATA_INTEGRITY,\n",
    "        \"integrity_type\": \"greater_than\",\n",
    "        \"threshold\": 100,\n",
    "        \"measurable_args\": {\n",
    "            'type': uptrain.MeasurableType.CUSTOM,\n",
    "            'signal_formulae': uptrain.Signal(\"Body Length\", body_length_signal),\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.Anomaly.EDGE_CASE, \n",
    "        \"signal_formulae\": uptrain.Signal(\"Pushup\", pushup_signal)\n",
    "\n",
    "    },\n",
    "    {\n",
    "        'type': uptrain.Anomaly.CONCEPT_DRIFT,\n",
    "        'algorithm': uptrain.DataDriftAlgo.DDM\n",
    "    }],\n",
    "\n",
    "    \"data_identifier\": \"id\",\n",
    "    \"feat_name_list\": [\"kps\"],\n",
    "\n",
    "    # Connect training pipeline to annotate data and retrain the model\n",
    "    \"training_args\": {\n",
    "        \"data_transformation_func\": input_to_dataset_transformation,  \n",
    "        \"annotation_method\": {\"method\": uptrain.AnnotationMethod.MASTER_FILE, \"args\": annotation_args}, \n",
    "        \"training_func\": train_model_torch, \n",
    "        \"fold_name\": data_save_fold,\n",
    "        \"orig_training_file\": orig_training_file,\n",
    "        \"cluster_plot_func\": plot_all_cluster\n",
    "    },\n",
    "\n",
    "    # Retrain once 250 edge cases are collected\n",
    "    \"retrain_after\": 250,\n",
    "\n",
    "    # Connect evaluation pipeline to test retrained model against original model\n",
    "    \"evaluation_args\": {\n",
    "        \"inference_func\": get_accuracy_torch,\n",
    "        \"golden_testing_dataset\": golden_testing_file,\n",
    "        \"metrics_to_check\": ['accuracy']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To integrate UpTrain, we need to just initialise a Framework object with above-defined config and log model inputs and outputs in our inference function. To monitor concept drift, we will also extract ground truth from annotation file and log GTs.\n",
    "\n",
    "To mimic real-world settings, we take a real-world testing dataset, load data-points batch by batch and run the model inference on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_smart_data\n",
      "Deleting the folder:  uptrain_logs\n",
      "55  edge-cases collected out of  208  inferred samples\n",
      "100  edge-cases collected out of  416  inferred samples\n",
      "151  edge-cases collected out of  624  inferred samples\n",
      "206  edge-cases collected out of  832  inferred samples\n",
      "250  edge-cases collected out of  992  inferred samples\n",
      "Kicking off re-training\n",
      "255 data-points selected out of 1008\n",
      "Training on:  uptrain_smart_data/1/training_dataset.json  which has  2275  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Model retraining done...\n",
      "Generating comparison report...\n",
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Evaluating on  15731  data-points\n",
      "Evaluating on  15731  data-points\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Old model accuracy:  0.8841777382238891\n",
      "Retrained model accuracy (ie 255 smartly collected data-points added):  0.992498887546882\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "framework_torch = uptrain.Framework(cfg)\n",
    "\n",
    "model_dir = 'trained_models_torch/'\n",
    "model_save_name = 'version_0'\n",
    "real_world_dataset = KpsDataset(\n",
    "    real_world_test_cases, batch_size=inference_batch_size, is_test=True\n",
    ")\n",
    "model = BinaryClassification()\n",
    "model.load_state_dict(torch.load(model_dir + model_save_name))\n",
    "model.eval()\n",
    "gt_data = read_json(annotation_args['master_file'])\n",
    "all_gt_ids = [x['id'] for x in gt_data]\n",
    "\n",
    "for i,elem in enumerate(real_world_dataset):\n",
    "\n",
    "    # Do model prediction\n",
    "    inputs = {\"data\": {\"kps\": elem[0][\"kps\"]}, \"id\": elem[0][\"id\"]}\n",
    "    x_test = torch.tensor(inputs[\"data\"][\"kps\"]).type(torch.float)\n",
    "    test_logits = model(x_test).squeeze() \n",
    "    preds = torch.round(torch.sigmoid(test_logits)).detach().numpy()\n",
    "\n",
    "    # Log model inputs and outputs to the uptrain Framework to monitor input and output data related checks\n",
    "    idens = framework_torch.log(inputs=inputs, outputs=preds)\n",
    "\n",
    "    # Attach ground truth to monitor model performance and concept drift\n",
    "    this_elem_gt = [gt_data[all_gt_ids.index(x)]['gt'] for x in elem[0]['id']]\n",
    "    framework_torch.log(identifiers=idens, gts=this_elem_gt)\n",
    "\n",
    "    # Retrain only once\n",
    "    if framework_torch.version > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.11.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir uptrain_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
