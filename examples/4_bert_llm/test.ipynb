{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a671f8cf-2293-4405-b07c-a5182863f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66cac4cb-b1a1-4b31-b2fa-0a1400cfc19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/transformers/installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61005f8-27ae-4a7a-a190-fe5f335821cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from model_constants import *\n",
    "from model_train import retrain_model\n",
    "from helper_funcs import *\n",
    "import json\n",
    "import uptrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6e1263-e704-4de2-af99-507ea68125fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "testing_text = \"Nike shoes are very [MASK].\"\n",
    "original_model_outputs = test_model(model, testing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9edc5da-8366-4c55-93a7-4601c89c9970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_smart_data_bert\n"
     ]
    }
   ],
   "source": [
    "def nike_text_present_func(inputs, outputs, gts=None, extra_args={}):\n",
    "    is_present = []\n",
    "    for input in inputs[\"text\"]:\n",
    "        this_present = \"nike\" in input.lower()\n",
    "        is_present.append(bool(this_present))\n",
    "    return is_present\n",
    "\n",
    "uptrain_save_fold_name = \"uptrain_smart_data_bert\"\n",
    "nike_text_present = uptrain.Signal(\"Nike Text Present\", nike_text_present_func)\n",
    "\n",
    "cfg = {\n",
    "    'checks': [{\n",
    "        'type': uptrain.Anomaly.EDGE_CASE,\n",
    "        \"signal_formulae\": nike_text_present\n",
    "    }],\n",
    "\n",
    "    'retraining_folder': uptrain_save_fold_name,\n",
    "}\n",
    "\n",
    "framework = uptrain.Framework(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe66970-c0ce-421d-9f7a-845453615903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  edge cases identified out of  197  total samples\n",
      "100  edge cases identified out of  397  total samples\n",
      "150  edge cases identified out of  597  total samples\n",
      "200  edge cases identified out of  797  total samples\n",
      "250  edge cases identified out of  997  total samples\n",
      "300  edge cases identified out of  1197  total samples\n",
      "350  edge cases identified out of  1397  total samples\n",
      "400  edge cases identified out of  1597  total samples\n",
      "450  edge cases identified out of  1797  total samples\n",
      "500  edge cases identified out of  1997  total samples\n",
      "550  edge cases identified out of  2197  total samples\n",
      "600  edge cases identified out of  2397  total samples\n",
      "650  edge cases identified out of  2597  total samples\n",
      "700  edge cases identified out of  2797  total samples\n",
      "750  edge cases identified out of  2997  total samples\n",
      "800  edge cases identified out of  3197  total samples\n",
      "850  edge cases identified out of  3397  total samples\n",
      "900  edge cases identified out of  3597  total samples\n",
      "950  edge cases identified out of  3797  total samples\n",
      "1000  edge cases identified out of  3997  total samples\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = create_sample_dataset(\"data.json\")\n",
    "with open(raw_dataset) as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "for sample in all_data['data']:\n",
    "    inputs = {'data': {'text': [sample['text']]}}\n",
    "    framework.log(inputs = inputs, outputs = None)\n",
    "\n",
    "retraining_dataset = create_dataset_from_csv(uptrain_save_fold_name + \"/1/smart_data.csv\", \"text\", \"retrain_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8d91d9-4540-4850-ad27-69fe9468e7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7b38c5d8a445c9cb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Downloading and preparing dataset json/default to /Users/sourabhagrawal/.cache/huggingface/datasets/json/default-7b38c5d8a445c9cb/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1114ee3f344b8d8e4e04f134f02c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36311552c2143cdbe3a580edf5684c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/sourabhagrawal/.cache/huggingface/datasets/json/default-7b38c5d8a445c9cb/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c09d3a0c08a48d795beea61155c2da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df783dbf6e51409c9cc4878306f71e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ea73c68886447a9f986a136dae4fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/sourabhagrawal/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 93\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n",
      "  Number of trainable parameters = 66985530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Before training, Perplexity: 7.20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.113000</td>\n",
       "      <td>1.247291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.451700</td>\n",
       "      <td>1.130067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.151200</td>\n",
       "      <td>0.848002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>After trainign, Perplexity: 2.56\n"
     ]
    }
   ],
   "source": [
    "retrain_model(model, retraining_dataset)\n",
    "retrained_model_outputs = test_model(model, testing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f014895c-0e6d-4886-a9b4-7071154e6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([original_model_outputs, retrained_model_outputs])\n",
    "\n",
    "# # Create Nike review training dataset\n",
    "# nike_attrs = {\n",
    "#     \"version\": \"0.1.0\",\n",
    "#     'source': \"nike review dataset\",\n",
    "#     'url': 'https://www.kaggle.com/datasets/tinkuzp23/nike-onlinestore-customer-reviews?resource=download',\n",
    "# }\n",
    "# # Download the dataset from the url, zip it and copy the csv file here\n",
    "# raw_nike_reviews_dataset = create_dataset_from_csv(\"web_scrapped.csv\", \"Content\", \"raw_nike_reviews_data.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
